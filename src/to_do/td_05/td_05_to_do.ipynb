{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "597Hu82gSNCZ"
      },
      "source": [
        "<img src=\"https://upload.wikimedia.org/wikipedia/fr/0/0b/Polytech_Lyon_logo.png\" alt=\"drawing\" height=\"200\"/>\n",
        "\n",
        "# Traitement de donn√©es & Programmation en Python\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EA-XK862WiMt"
      },
      "source": [
        "# TD 05\n",
        "\n",
        "Traitement de donn√©es\n",
        "\n",
        "![Good luck!](https://media.tenor.com/YoFWnXe4V3kAAAAd/may-the-odds-be-ever-in-your-favor-may-the-odds-hunger-games.gif)\n",
        " \n",
        "Elements √† consulter:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Doc                                   |             Link\n",
        "--------------------------------------|------------------------------------\n",
        "Github Helper      | [>link<](#scrollTo=Github_101)\n",
        "Python en 30 jours | [>link<](https://moncoachdata.com/courses/apprendre-python-en-30-jours/)\n",
        "Get started with pandas | [>link<](https://colab.research.google.com/notebooks/snippets/pandas.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1g4EgT41MqJ"
      },
      "source": [
        "## Intro\n",
        "\n",
        "Le premier bloc devrait toujours contenir les installs/imports dont on aura besoin pour le reste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjiE8c51VoT0",
        "outputId": "2f4856bd-590b-4197-a9f0-e3a020d191eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python is awesome üëç\n"
          ]
        }
      ],
      "source": [
        "# Installs\n",
        "print(\"Python is awesome üëç\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhT-50uwz72F"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "import os\n",
        "import json\n",
        "from random import randint\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsB9ADLg8i8P",
        "outputId": "162e09d0-12e6-4112-972d-91cea118215a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2, 3)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = np.array([[1, 2, 3], [4, 5, 6]], np.int32)\n",
        "\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKow2OZ5LCXr"
      },
      "source": [
        "### Correction TD precedent\n",
        "\n",
        "Partir d'un fichier csv et alimenter la binge watch list :\n",
        "  > serie, plateform, nb_episodes, ann√©e_sortie, note\n",
        "\n",
        "Puis print :\n",
        "\n",
        "\t Nom de la s√©rie : Brooklyn Nine-Nine\n",
        "\t Ann√©e de sortie : 2013\n",
        "\n",
        "\t Nom de la s√©rie : The office\n",
        "\t Ann√©e de sortie : 2005\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9lcQQFp3w0HE"
      },
      "outputs": [
        {
          "ename": "UnicodeDecodeError",
          "evalue": "'charmap' codec can't decode byte 0x81 in position 2861: character maps to <undefined>",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [4], line 38\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[39m# file_path = \"./tv_shows_pipe.csv\"\u001b[39;00m\n\u001b[0;32m     37\u001b[0m limit \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m binge_watch_list \u001b[39m=\u001b[39m csv_to_dict(file_path,separator\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m|\u001b[39;49m\u001b[39m\"\u001b[39;49m,row_limit\u001b[39m=\u001b[39;49mlimit)\n\u001b[0;32m     39\u001b[0m \u001b[39m# for serie in binge_watch_list:\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[39m#   print(serie.get('title'))\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[39m# # Option 1 read some rows to see what we have\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[39m#               value_vars=['netflix','hulu','prime_video','disney+'])\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[39m# df2.drop(['value'], axis=1)\u001b[39;00m\n",
            "Cell \u001b[1;32mIn [4], line 16\u001b[0m, in \u001b[0;36mcsv_to_dict\u001b[1;34m(file_path, separator, row_limit)\u001b[0m\n\u001b[0;32m     14\u001b[0m serie \u001b[39m=\u001b[39m {}\n\u001b[0;32m     15\u001b[0m serie_list \u001b[39m=\u001b[39m []\n\u001b[1;32m---> 16\u001b[0m \u001b[39mfor\u001b[39;00m i, row \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(f, \u001b[39m1\u001b[39m):\n\u001b[0;32m     17\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m     18\u001b[0m         header \u001b[39m=\u001b[39m row\u001b[39m.\u001b[39mlower()\u001b[39m.\u001b[39mstrip()\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39msplit(separator)\n",
            "File \u001b[1;32mc:\\Users\\nicol\\anaconda3\\lib\\encodings\\cp1252.py:23\u001b[0m, in \u001b[0;36mIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, final\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m---> 23\u001b[0m     \u001b[39mreturn\u001b[39;00m codecs\u001b[39m.\u001b[39;49mcharmap_decode(\u001b[39minput\u001b[39;49m,\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merrors,decoding_table)[\u001b[39m0\u001b[39m]\n",
            "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x81 in position 2861: character maps to <undefined>"
          ]
        }
      ],
      "source": [
        "def csv_to_dict(file_path, separator=\";\", row_limit=None):\n",
        "    \"\"\"\n",
        "    Transforms a csv file, into a list of dictionaries\n",
        "\n",
        "    Arguments:\n",
        "        file_path  {str} : eg. /documents/file.txt\n",
        "        separator  {str} : separator of fields, default (;)\n",
        "        row_limit  {int} : limit the rows to be read, default None\n",
        "\n",
        "    Returns list[dict]\n",
        "    \"\"\"\n",
        "\n",
        "    with open(file_path, \"r\") as f:\n",
        "        serie = {}\n",
        "        serie_list = []\n",
        "        for i, row in enumerate(f, 1):\n",
        "            if i == 1:\n",
        "                header = row.lower().strip().replace(' ','_').split(separator)\n",
        "                # print(\"\\nHeaders:\", header,\"\\n\")\n",
        "            else:\n",
        "                data = row.lower().strip().split(separator)\n",
        "                # print(\"Data:\",data)\n",
        "                if len(header) == len(data):\n",
        "                    for j, element in enumerate(header):\n",
        "                        serie[header[j]] = data[j]\n",
        "            # print(serie)\n",
        "                serie_list.append(serie)\n",
        "                serie = {}\n",
        "            if i == row_limit:\n",
        "                break\n",
        "    return serie_list\n",
        "\n",
        "\n",
        "\n",
        "file_path = \"./netflix_titles.csv\"\n",
        "# file_path = \"./tv_shows_pipe.csv\"\n",
        "limit = None\n",
        "binge_watch_list = csv_to_dict(file_path,separator=\"|\",row_limit=limit)\n",
        "# for serie in binge_watch_list:\n",
        "#   print(serie.get('title'))\n",
        "# # Option 1 read some rows to see what we have\n",
        "\n",
        "#df = pd.DataFrame(binge_watch_list)\n",
        "#df[\"title\"] = df[\"title\"].str.title()\n",
        "#df.head(5)\n",
        "\n",
        "#column_name = 'release_year'\n",
        "#filter = df['release_year'] == '2005'\n",
        "# df.query(f\"`{column_name}` == '2003'\")\n",
        "#df[filter]\n",
        "# # serie, plateform, nb_episodes, ann√©e_sortie, note\n",
        "# df2 = df.melt(id_vars=['title','year','imdb','rotten_tomatoes'],\n",
        "#               var_name='plateform', \n",
        "#               value_vars=['netflix','hulu','prime_video','disney+'])\n",
        "# df2.drop(['value'], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2lJWv62kQZA"
      },
      "source": [
        "##EX01\n",
        "### Numpy intro\n",
        "\n",
        "* Cr√©er un vecteur depuis la liste ['Lyon', 'Paris', 'Montpellier']\n",
        "    * Assigner le r√©sultat √† la variable `villes`\n",
        "* Cr√©er une matrice depuis la liste de listes suivante: `[['Lyon', '69'], ['Paris', '75'], ['Montpellier','34']]`\n",
        "    * Assigner le r√©sultat √† la variable `villes_departement`\n",
        "* Assigner la taille du vecteur `villes` √† la variable `v_shape`\n",
        "* Assigner la taille du tableau `villes_departement` √† la variable `vd_shape` \n",
        "* Afficher les r√©sultats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "n_jOsgGllUhC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Lyon' 'Paris' 'Montpellier']\n",
            "[['Lyon' '69']\n",
            " ['Paris' '75']\n",
            " ['Montpellier' '34']]\n",
            "3\n",
            "6\n"
          ]
        }
      ],
      "source": [
        "villes = np.array(['Lyon','Paris','Montpellier'])\n",
        "villes_department = np.array([['Lyon','69'],['Paris','75'],['Montpellier','34']])\n",
        "v_shape = np.size(villes)\n",
        "vd_shape = np.size(villes_department)\n",
        "print(villes)\n",
        "print(villes_department)\n",
        "print(v_shape)\n",
        "print(vd_shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8WBS9oVKcWs"
      },
      "source": [
        "##EX02\n",
        "### Numpy intro\n",
        "\n",
        "#### Part 1\n",
        "\n",
        "* Telecharger le dataset [cereal.csv](https://www.kaggle.com/datasets/crawford/80-cereals?select=cereal.csv) avec les colonnes suivantes :\n",
        "\n",
        "```\n",
        "['name', 'mfr', 'type', 'calories', 'protein', 'fat', 'sodium', 'fiber',\n",
        "'carbo', 'sugars', 'potass', 'vitamins', 'shelf', 'weight', 'cups',\n",
        "'rating']\n",
        "```\n",
        "\n",
        "* Afficher les deux premieres colonnes et toute les lignes\n",
        "* Afficher la derni√®re colonne\n",
        "* Afficher les 5 premieres lignes de la 4√®me colonne\n",
        "* Assigner a une variable `corn_flakes_cals` le nombre de calories par 100 gr de c√©r√©ales de la marque Corn Flakes \n",
        "* Assigner le nom de la 3eme marque sur le dataset a une variabe `third_brand`\n",
        "* Mettre dans une liste toutes les marques de c√©reales en utilisant `.tolist()` & afficher cette derni√®re\n",
        "* Mettre les 10 premiere marques dans le fichiers dans une liste `first_ten_brands` & afficher cette derni√®re\n",
        "> bonus \n",
        "* Convertir la colonne ratings en d√©cimale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTDsqYIgpitx",
        "outputId": "d2cd6d75-cf4d-4421-c34f-097def1f3b94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(78, 16)\n"
          ]
        }
      ],
      "source": [
        "# Code here\n",
        "file_path = \"./cereal.csv\"\n",
        "data = np.genfromtxt(file_path,delimiter=',',dtype='str',autostrip=True)\n",
        "# print(data[:,:1])\n",
        "# print(data[data.shape[0]-1,:])\n",
        "# print(data[:4,3])\n",
        "\n",
        "corn_flakes = (data[:,0] == 'Corn Flakes')\n",
        "corn_flakes_cals = data[corn_flakes, 3]\n",
        "# print(corn_flakes_cals)\n",
        "\n",
        "third_brand = data[3,0]\n",
        "# print(third_brand)\n",
        "\n",
        "brands = data[1:,0].tolist()\n",
        "# print(brands)\n",
        "\n",
        "first_ten_brands = data[1:11,0]\n",
        "# print(first_ten_brands)\n",
        "\n",
        "#print(data[1:,15].astype(float))\n",
        "print(data.shape) ## retourne (lignes, colonnes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BaW1VvWXCS7"
      },
      "source": [
        "#### Part 2\n",
        "\n",
        "* **Extraire** la premiere colonne du dataset et **la comparer** √† la valeur `'Special K'` Assigner le resultat √† une variable `special_k`\n",
        "* Executer la commande `print(data[special_k])`\n",
        "> bonus\n",
        "* Faire la meme chose mais le filtre verifie que le manufacturer est Quaker Oats (lire la doc du dataset üòâ)\n",
        "> bonus ¬≤\n",
        "* Print le nom des marques dont le fabricant est `Quaker Oats` et qui contiennent **moins** de `10 grammes` de sucre par portion\n",
        "* Ajouter une marque de c√©r√©ales \"healthy\" avec 0 grammes de sucre au dataset "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GuXFyhUepnMc",
        "outputId": "6ec9c05d-a76c-4d8f-8c48-6285d32b6953"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['name' 'mfr' 'type' ... 'weight' 'cups' 'rating']\n",
            " ['100% Bran' 'N' 'C' ... '1' '0.33' '68.402973']\n",
            " ['100% Natural Bran' 'Q' 'C' ... '1' '1' '33.983679']\n",
            " ...\n",
            " ['Wheaties' 'G' 'C' ... '1' '1' '51.592193']\n",
            " ['Wheaties Honey Gold' 'G' 'C' ... '1' '0.75' '36.187559']\n",
            " ['healthy' '' '' ... '' '' '']]\n"
          ]
        }
      ],
      "source": [
        "special_k = (data[1:,0] == 'Special K')\n",
        "# print(special_k)\n",
        "\n",
        "\n",
        "quaker_oaks = (data[1:,1] == 'Q')\n",
        "# print(quaker_oaks)\n",
        "\n",
        "quaker_dix_g = (quaker_oaks & (data[1:,9].astype(int) <= 10))\n",
        "# print(quaker_dix_g)\n",
        "\n",
        "healthy = np.array(['healthy','','','','','','','','','0','','','','','',''])\n",
        "data = np.vstack([data,healthy])\n",
        "print(data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdsY64gl_0dw"
      },
      "source": [
        "#### Part 3\n",
        "\n",
        "Sur le m√™me dataset, √† l'aide d'une boucle `for`:\n",
        "\n",
        "\n",
        "*   Cr√©er un dictionnaire avec la moyenne de sucre par Fabricant de c√©rales\n",
        "*   Trouver la marque de c√©reales qui contient le plus de sucre par portion\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjbTgYNbCaIj",
        "outputId": "5ab1f623-f8df-45f2-e82b-7031da73f3f5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([['100% Bran', 'Nabisco', 'C', ..., '1', '0.33', '68.402973'],\n",
              "       ['100% Natural Bran', 'Quaker Oats', 'C', ..., '1', '1',\n",
              "        '33.983679'],\n",
              "       ['All-Bran', 'Kelloggs', 'C', ..., '1', '0.33', '59.425505'],\n",
              "       ...,\n",
              "       ['Wheat Chex', 'Ralston Purina', 'C', ..., '1', '0.67',\n",
              "        '49.787445'],\n",
              "       ['Wheaties', 'General Mills', 'C', ..., '1', '1', '51.592193'],\n",
              "       ['Wheaties Honey Gold', 'General Mills', 'C', ..., '1', '0.75',\n",
              "        '36.187559']], dtype='<U75')"
            ]
          },
          "execution_count": 469,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1wjqra3_yZo",
        "outputId": "ea12a660-30c6-45d0-e066-c7f4848eb455"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'Ralston Purina': 6.125, 'Nabisco': 1.8333333333333333, 'General Mills': 7.954545454545454, 'Kelloggs': 7.565217391304348, 'American Home Food Products': 3.0, 'Post': 8.777777777777779, 'Quaker Oats': 5.25}\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHsrGdy-5AZK"
      },
      "source": [
        "## EX03\n",
        "### Dataframes\n",
        "\n",
        "Refaire l'exo du TD 4 (fichier des series) en utilisant la librairie NumPy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIatRWhm5Xzo"
      },
      "outputs": [],
      "source": [
        "# Code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94-12vgPXG10"
      },
      "source": [
        "## Github 101\n",
        "\n",
        "```shell\n",
        "# Repartir sur une nouvelle branche\n",
        "git checkout main\n",
        "# Mettre √† jour notre branche locale\n",
        "git pull origin main\n",
        "# Recr√©er une branche pour le TD 05\n",
        "git checkout -b branch_name_td05\n",
        "# Dupliquer le fichier du td et rajouter en suffix son nom\n",
        "cd 'chemin/m1-miage/'\n",
        "cp 'src/to_do/td_05/td_05.ipynb' 'src/done/td_05/td_05_nom_prenom.ipynb'\n",
        "# Faire le travail necessaire sur votre fichier et commit et push\n",
        "git add 'src/done/td_05/td_05_nom_prenom.ipynb'\n",
        "git commit -m 'message descriptif du travail fait'\n",
        "git push --set-upstream origin nom_branche\n",
        "# Faire une pull request\n",
        "```\n",
        "\n",
        "[>>>PULL REQUEST<<<](https://github.com/lapointe05/m1-miage/pulls)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "597Hu82gSNCZ",
        "k2lJWv62kQZA",
        "94-12vgPXG10"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "07878c45a9b18549480ac2f9ecb8f16bb7162456cf654f6757bcff1d6396abe7"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
