{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "597Hu82gSNCZ"
   },
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/fr/0/0b/Polytech_Lyon_logo.png\" alt=\"drawing\" height=\"200\"/>\n",
    "\n",
    "# Traitement de donn√©es & Programmation en Python\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EA-XK862WiMt"
   },
   "source": [
    "# TD 05\n",
    "\n",
    "Traitement de donn√©es\n",
    "\n",
    "![Good luck!](https://media.tenor.com/YoFWnXe4V3kAAAAd/may-the-odds-be-ever-in-your-favor-may-the-odds-hunger-games.gif)\n",
    " \n",
    "Elements √† consulter:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Doc                                   |             Link\n",
    "--------------------------------------|------------------------------------\n",
    "Github Helper      | [>link<](#scrollTo=Github_101)\n",
    "Python en 30 jours | [>link<](https://moncoachdata.com/courses/apprendre-python-en-30-jours/)\n",
    "Get started with pandas | [>link<](https://colab.research.google.com/notebooks/snippets/pandas.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y1g4EgT41MqJ"
   },
   "source": [
    "## Intro\n",
    "\n",
    "Le premier bloc devrait toujours contenir les installs/imports dont on aura besoin pour le reste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yjiE8c51VoT0",
    "outputId": "2f4856bd-590b-4197-a9f0-e3a020d191eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python is awesome üëç\n"
     ]
    }
   ],
   "source": [
    "# Installs\n",
    "print(\"Python is awesome üëç\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "JhT-50uwz72F"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import json\n",
    "from random import randint\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NsB9ADLg8i8P",
    "outputId": "162e09d0-12e6-4112-972d-91cea118215a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[1, 2, 3], [4, 5, 6]], np.int32)\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iKow2OZ5LCXr"
   },
   "source": [
    "### Correction TD precedent\n",
    "\n",
    "Partir d'un fichier csv et alimenter la binge watch list :\n",
    "  > serie, plateform, nb_episodes, ann√©e_sortie, note\n",
    "\n",
    "Puis print :\n",
    "\n",
    "\t Nom de la s√©rie : Brooklyn Nine-Nine\n",
    "\t Ann√©e de sortie : 2013\n",
    "\n",
    "\t Nom de la s√©rie : The office\n",
    "\t Ann√©e de sortie : 2005\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "9lcQQFp3w0HE"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './netflix_titles.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-c825503ebaf1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;31m# file_path = \"./tv_shows_pipe.csv\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[0mlimit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m \u001b[0mbinge_watch_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv_to_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mseparator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"|\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrow_limit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;31m# for serie in binge_watch_list:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;31m#   print(serie.get('title'))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-c825503ebaf1>\u001b[0m in \u001b[0;36mcsv_to_dict\u001b[1;34m(file_path, separator, row_limit)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \"\"\"\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0mserie\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mserie_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './netflix_titles.csv'"
     ]
    }
   ],
   "source": [
    "def csv_to_dict(file_path, separator=\";\", row_limit=None):\n",
    "    \"\"\"\n",
    "    Transforms a csv file, into a list of dictionaries\n",
    "\n",
    "    Arguments:\n",
    "        file_path  {str} : eg. /documents/file.txt\n",
    "        separator  {str} : separator of fields, default (;)\n",
    "        row_limit  {int} : limit the rows to be read, default None\n",
    "\n",
    "    Returns list[dict]\n",
    "    \"\"\"\n",
    "\n",
    "    with open(file_path, \"r\") as f:\n",
    "        serie = {}\n",
    "        serie_list = []\n",
    "        for i, row in enumerate(f, 1):\n",
    "            if i == 1:\n",
    "                header = row.lower().strip().replace(' ','_').split(separator)\n",
    "                # print(\"\\nHeaders:\", header,\"\\n\")\n",
    "            else:\n",
    "                data = row.lower().strip().split(separator)\n",
    "                # print(\"Data:\",data)\n",
    "                if len(header) == len(data):\n",
    "                    for j, element in enumerate(header):\n",
    "                        serie[header[j]] = data[j]\n",
    "            # print(serie)\n",
    "                serie_list.append(serie)\n",
    "                serie = {}\n",
    "            if i == row_limit:\n",
    "                break\n",
    "    return serie_list\n",
    "\n",
    "\n",
    "\n",
    "file_path = \"./netflix_titles.csv\"\n",
    "# file_path = \"./tv_shows_pipe.csv\"\n",
    "limit = None\n",
    "binge_watch_list = csv_to_dict(file_path,separator=\"|\",row_limit=limit)\n",
    "# for serie in binge_watch_list:\n",
    "#   print(serie.get('title'))\n",
    "# # Option 1 read some rows to see what we have\n",
    "\n",
    "df = pd.DataFrame(binge_watch_list)\n",
    "df[\"title\"] = df[\"title\"].str.title()\n",
    "df.head(5)\n",
    "\n",
    "column_name = 'release_year'\n",
    "filter = df['release_year'] == '2005'\n",
    "# df.query(f\"`{column_name}` == '2003'\")\n",
    "df[filter]\n",
    "# # serie, plateform, nb_episodes, ann√©e_sortie, note\n",
    "# df2 = df.melt(id_vars=['title','year','imdb','rotten_tomatoes'],\n",
    "#               var_name='plateform', \n",
    "#               value_vars=['netflix','hulu','prime_video','disney+'])\n",
    "# df2.drop(['value'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k2lJWv62kQZA"
   },
   "source": [
    "##EX01\n",
    "### Numpy intro\n",
    "\n",
    "* Cr√©er un vecteur depuis la liste ['Lyon', 'Paris', 'Montpellier']\n",
    "    * Assigner le r√©sultat √† la variable `villes`\n",
    "* Cr√©er une matrice depuis la liste de listes suivante: `[['Lyon', '69'], ['Paris', '75'], ['Montpellier','34']]`\n",
    "    * Assigner le r√©sultat √† la variable `villes_departement`\n",
    "* Assigner la taille du vecteur `villes` √† la variable `v_shape`\n",
    "* Assigner la taille du tableau `villes_departement` √† la variable `vd_shape` \n",
    "* Afficher les r√©sultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "n_jOsgGllUhC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voici le vecteur villes: ['Lyon' 'Paris' 'Montpellier']\n",
      "\n",
      "\n",
      "Voici le vecteur villes d√©partement: [['Lyon' '69']\n",
      " ['Paris' '75']\n",
      " ['Montpellier' '34']]\n",
      "\n",
      "\n",
      "Le tableau villes a une taille de 3 et villes d√©partement une taille de 3.\n"
     ]
    }
   ],
   "source": [
    "# Liste villes\n",
    "\n",
    "liste_villes = ['Lyon','Paris','Montpellier']\n",
    "\n",
    "villes = np.array(liste_villes)\n",
    "\n",
    "print(f'Voici le vecteur villes: {villes}\\n\\n')\n",
    "\n",
    "\n",
    "# Liste villes d√©partements\n",
    "\n",
    "\n",
    "mat_villes_dep = [['Lyon', '69'], ['Paris', '75'], ['Montpellier','34']]\n",
    "\n",
    "villes_departement = np.array(mat_villes_dep)\n",
    "\n",
    "print(f'Voici le vecteur villes d√©partement: {villes_departement}\\n\\n')\n",
    "\n",
    "\n",
    "# Tailles\n",
    "\n",
    "\n",
    "v_shape = villes.shape[0]\n",
    "\n",
    "vd_shape = villes_departement.shape[0]\n",
    "\n",
    "print(f'Le tableau villes a une taille de {v_shape} et villes d√©partement une taille de {vd_shape}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q8WBS9oVKcWs"
   },
   "source": [
    "##EX02\n",
    "### Numpy intro\n",
    "\n",
    "#### Part 1\n",
    "\n",
    "* Telecharger le dataset [cereal.csv](https://www.kaggle.com/datasets/crawford/80-cereals?select=cereal.csv) avec les colonnes suivantes :\n",
    "\n",
    "```\n",
    "['name', 'mfr', 'type', 'calories', 'protein', 'fat', 'sodium', 'fiber',\n",
    "'carbo', 'sugars', 'potass', 'vitamins', 'shelf', 'weight', 'cups',\n",
    "'rating']\n",
    "```\n",
    "\n",
    "* Afficher les deux premieres colonnes et toute les lignes\n",
    "* Afficher la derni√®re colonne\n",
    "* Afficher les 5 premieres lignes de la 4√®me colonne\n",
    "* Assigner a une variable `corn_flakes_cals` le nombre de calories par 100 gr de c√©r√©ales de la marque Corn Flakes \n",
    "* Assigner le nom de la 3eme marque sur le dataset a une variabe `third_brand`\n",
    "* Mettre dans une liste toutes les marques de c√©reales en utilisant `.tolist()` & afficher cette derni√®re\n",
    "* Mettre les 10 premiere marques dans le fichiers dans une liste `first_ten_brands` & afficher cette derni√®re\n",
    "> bonus \n",
    "* Convertir la colonne ratings en d√©cimale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XTDsqYIgpitx",
    "outputId": "d2cd6d75-cf4d-4421-c34f-097def1f3b94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les deux premiers √©l√©ments sont:\n",
      "[['name' 'mfr' 'type' 'calories' 'protein' 'fat' 'sodium' 'fiber' 'carbo'\n",
      "  'sugars' 'potass' 'vitamins' 'shelf' 'weight' 'cups' 'rating']\n",
      " ['100% Bran' 'N' 'C' '70' '4' '1' '130' '10' '5' '6' '280' '25' '3' '1'\n",
      "  '0.33' '68.402973']]\n",
      "\n",
      "Voici la derni√®re colonne:\n",
      "rating\n",
      "\n",
      "Les cinq premi√®res lignes de la quatri√®me colonne sont:\n",
      "['calories' '70' '120' '70' '50']\n",
      "\n",
      "Pour 100g, les c√©r√©ales de la marque Corn Flakes ont 1 cal.\n",
      "\n",
      "Le nom de la troisi√®me marque est All-Bran.\n"
     ]
    }
   ],
   "source": [
    "    # Code here\n",
    "    \n",
    "file_path = \"C:/Users/a.aulagnier/Documents/Cours/MIAGE/Data/cereal.csv\" # recompile pas stp\n",
    "\n",
    "data = np.genfromtxt(\n",
    "    \n",
    "                      file_path,\n",
    "    \n",
    "    dtype = None,\n",
    "    \n",
    "    delimiter = ',',\n",
    "    \n",
    "    filling_values = None,\n",
    "    \n",
    "    # names = True # le param√®tre avait l'air cool mais √ßa retournait des tuples dans une array T_T √ßa m'a pris 1h\n",
    "    \n",
    "    encoding = None\n",
    "    \n",
    "                     )\n",
    "\n",
    "print(f'Les deux premiers √©l√©ments sont:\\n{data[:2]}\\n') # retourne les deux premiers √©l√©ments\n",
    "\n",
    "print(f'Voici la derni√®re colonne:\\n{data[0,-1]}\\n') # je savais pas si c'√©tait √ßa ou data[:,-1] (un peu moins beau, mais plus exhaustif ^^'')\n",
    "\n",
    "print(f'Les cinq premi√®res lignes de la quatri√®me colonne sont:\\n{data[:5, 3]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pour 100g, les c√©r√©ales de la marque Corn Flakes ont 1 cal.\n"
     ]
    }
   ],
   "source": [
    "corn_flakes_cals = int(data[(data[:,0]=='Corn Flakes'),3])//100\n",
    "\n",
    "print(f'\\nPour 100g, les c√©r√©ales de la marque Corn Flakes ont {corn_flakes_cals} cal.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Le nom de la troisi√®me marque est All-Bran.\n"
     ]
    }
   ],
   "source": [
    "third_brand = data[3,(data[0,:]=='name')]\n",
    "\n",
    "print(f'\\nLe nom de la troisi√®me marque est {third_brand[0]}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cette fonction est utile pour transformer des listes de listes en listes (une dimension), cas rencontr√© ci-dessous\n",
    "\n",
    "def flatten_list(l):\n",
    "    \n",
    "    flat_list = [item for sublist in l for item in sublist]\n",
    "    \n",
    "    return flat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voici la liste de toutes les marques: ['100% Bran', '100% Natural Bran', 'All-Bran', 'All-Bran with Extra Fiber', 'Almond Delight', 'Apple Cinnamon Cheerios', 'Apple Jacks', 'Basic 4', 'Bran Chex', 'Bran Flakes', \"Cap'n'Crunch\", 'Cheerios', 'Cinnamon Toast Crunch', 'Clusters', 'Cocoa Puffs', 'Corn Chex', 'Corn Flakes', 'Corn Pops', 'Count Chocula', \"Cracklin' Oat Bran\", 'Cream of Wheat (Quick)', 'Crispix', 'Crispy Wheat & Raisins', 'Double Chex', 'Froot Loops', 'Frosted Flakes', 'Frosted Mini-Wheats', 'Fruit & Fibre Dates; Walnuts; and Oats', 'Fruitful Bran', 'Fruity Pebbles', 'Golden Crisp', 'Golden Grahams', 'Grape Nuts Flakes', 'Grape-Nuts', 'Great Grains Pecan', 'Honey Graham Ohs', 'Honey Nut Cheerios', 'Honey-comb', 'Just Right Crunchy  Nuggets', 'Just Right Fruit & Nut', 'Kix', 'Life', 'Lucky Charms', 'Maypo', 'Muesli Raisins; Dates; & Almonds', 'Muesli Raisins; Peaches; & Pecans', 'Mueslix Crispy Blend', 'Multi-Grain Cheerios', 'Nut&Honey Crunch', 'Nutri-Grain Almond-Raisin', 'Nutri-grain Wheat', 'Oatmeal Raisin Crisp', 'Post Nat. Raisin Bran', 'Product 19', 'Puffed Rice', 'Puffed Wheat', 'Quaker Oat Squares', 'Quaker Oatmeal', 'Raisin Bran', 'Raisin Nut Bran', 'Raisin Squares', 'Rice Chex', 'Rice Krispies', 'Shredded Wheat', \"Shredded Wheat 'n'Bran\", 'Shredded Wheat spoon size', 'Smacks', 'Special K', 'Strawberry Fruit Wheats', 'Total Corn Flakes', 'Total Raisin Bran', 'Total Whole Grain', 'Triples', 'Trix', 'Wheat Chex', 'Wheaties', 'Wheaties Honey Gold']\n"
     ]
    }
   ],
   "source": [
    "all_brands = flatten_list(data[1:,(data[0,:]=='name')].tolist())\n",
    "\n",
    "print(f'Voici la liste de toutes les marques: {all_brands}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Voici les dix premi√®res marques: ['100% Bran', '100% Natural Bran', 'All-Bran', 'All-Bran with Extra Fiber', 'Almond Delight', 'Apple Cinnamon Cheerios', 'Apple Jacks', 'Basic 4', 'Bran Chex']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "first_ten_brands = flatten_list(data[1:10,(data[0,:]=='name')].tolist())\n",
    "\n",
    "print(f'\\nVoici les dix premi√®res marques: {first_ten_brands}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68.40\n",
      "33.98\n",
      "59.43\n",
      "93.70\n",
      "34.38\n",
      "29.51\n",
      "33.17\n",
      "37.04\n",
      "49.12\n",
      "53.31\n",
      "18.04\n",
      "50.76\n",
      "19.82\n",
      "40.40\n",
      "22.74\n",
      "41.45\n",
      "45.86\n",
      "35.78\n",
      "22.40\n",
      "40.45\n",
      "64.53\n",
      "46.90\n",
      "36.18\n",
      "44.33\n",
      "32.21\n",
      "31.44\n",
      "58.35\n",
      "40.92\n",
      "41.02\n",
      "28.03\n",
      "35.25\n",
      "23.80\n",
      "52.08\n",
      "53.37\n",
      "45.81\n",
      "21.87\n",
      "31.07\n",
      "28.74\n",
      "36.52\n",
      "36.47\n",
      "39.24\n",
      "45.33\n",
      "26.73\n",
      "54.85\n",
      "37.14\n",
      "34.14\n",
      "30.31\n",
      "40.11\n",
      "29.92\n",
      "40.69\n",
      "59.64\n",
      "30.45\n",
      "37.84\n",
      "41.50\n",
      "60.76\n",
      "63.01\n",
      "49.51\n",
      "50.83\n",
      "39.26\n",
      "39.70\n",
      "55.33\n",
      "42.00\n",
      "40.56\n",
      "68.24\n",
      "74.47\n",
      "72.80\n",
      "31.23\n",
      "53.13\n",
      "59.36\n",
      "38.84\n",
      "28.59\n",
      "46.66\n",
      "39.11\n",
      "27.75\n",
      "49.79\n",
      "51.59\n",
      "36.19\n"
     ]
    }
   ],
   "source": [
    "from decimal import Decimal # avec cette biblioth√®que, nous allons pouvoir changer la cha√Æne de caract√®res en d√©cimal\n",
    "# avec 2 chiffres apr√®s la virgule\n",
    "\n",
    "notes = flatten_list(data[1:,(data[0,:]=='rating')].tolist())\n",
    "\n",
    "notes_dec = []\n",
    "\n",
    "for i in range(len(notes)):\n",
    "    \n",
    "    inote = Decimal(notes[i])\n",
    "\n",
    "    notes_dec.append(round(inote,2)) # On r√©duit √† 2 d√©cimales pour une meilleure lisibilit√©\n",
    "\n",
    "\n",
    "for j in notes_dec:\n",
    "    \n",
    "    print(j) # Les notes sont affich√©es comme ligne par ligne\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8BaW1VvWXCS7"
   },
   "source": [
    "#### Part 2\n",
    "\n",
    "* **Extraire** la premiere colonne du dataset et **la comparer** √† la valeur `'Special K'` Assigner le resultat √† une variable `special_k`\n",
    "* Executer la commande `print(data[special_k])`\n",
    "> bonus\n",
    "* Faire la meme chose mais le filtre verifie que le manufacturer est Quaker Oats (lire la doc du dataset üòâ)\n",
    "> bonus ¬≤\n",
    "* Print le nom des marques dont le fabricant est `Quaker Oats` et qui contiennent **moins** de `10 grammes` de sucre par portion\n",
    "* Ajouter une marque de c√©r√©ales \"healthy\" avec 0 grammes de sucre au dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GuXFyhUepnMc",
    "outputId": "6ec9c05d-a76c-4d8f-8c48-6285d32b6953"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Voici les informations de Special K: [['Special K', 'K', 'C', '110', '6', '0', '230', '1', '16', '3', '55', '25', '1', '1', '1', '53.131324']]\n",
      "\n",
      "Avec le flatenning de liste: ['Special K', 'K', 'C', '110', '6', '0', '230', '1', '16', '3', '55', '25', '1', '1', '1', '53.131324']\n"
     ]
    }
   ],
   "source": [
    "# Special K <<<< Miel Pop's mais bon\n",
    "\n",
    "special_k = (data[:,0] == 'Special K') # Comparaison de tous les premiers √©l√©ments(ie les colonnes) √† 'Special K'\n",
    "\n",
    "print(f'\\nVoici les informations de Special K: {data[special_k].tolist()}')\n",
    "\n",
    "print(f'\\nAvec le flatenning de liste: {flatten_list(data[special_k])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Voici l'ensemble des c√©r√©ales dont le fabricant est Quaker Oats: [['100% Natural Bran' 'Q' 'C' '120' '3' '5' '15' '2' '8' '8' '135' '0'\n",
      "  '3' '1' '1' '33.983679']\n",
      " [\"Cap'n'Crunch\" 'Q' 'C' '120' '1' '2' '220' '0' '12' '12' '35' '25' '2'\n",
      "  '1' '0.75' '18.042851']\n",
      " ['Honey Graham Ohs' 'Q' 'C' '120' '1' '2' '220' '1' '12' '11' '45' '25'\n",
      "  '2' '1' '1' '21.871292']\n",
      " ['Life' 'Q' 'C' '100' '4' '2' '150' '2' '12' '6' '95' '25' '2' '1'\n",
      "  '0.67' '45.328074']\n",
      " ['Puffed Rice' 'Q' 'C' '50' '1' '0' '0' '0' '13' '0' '15' '0' '3' '0.5'\n",
      "  '1' '60.756112']\n",
      " ['Puffed Wheat' 'Q' 'C' '50' '2' '0' '0' '1' '10' '0' '50' '0' '3' '0.5'\n",
      "  '1' '63.005645']\n",
      " ['Quaker Oat Squares' 'Q' 'C' '100' '4' '1' '135' '2' '14' '6' '110'\n",
      "  '25' '3' '1' '0.5' '49.511874']\n",
      " ['Quaker Oatmeal' 'Q' 'H' '100' '5' '2' '0' '2.7' '-1' '-1' '110' '0'\n",
      "  '1' '1' '0.67' '50.828392']]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "manufacturer = (data[0,:] == 'mfr')\n",
    "\n",
    "quaker_oats = flatten_list((data[:,manufacturer] == 'Q'))\n",
    "\n",
    "print(f\"\\n Voici l'ensemble des c√©r√©ales dont le fabricant est Quaker Oats: {data[quaker_oats]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['100% Natural Bran' 'Q' 'C' '120' '3' '5' '15' '2' '8' '8' '135' '0' '3'\n",
      " '1' '1' '33.983679']\n",
      "['Life' 'Q' 'C' '100' '4' '2' '150' '2' '12' '6' '95' '25' '2' '1' '0.67'\n",
      " '45.328074']\n",
      "['Puffed Rice' 'Q' 'C' '50' '1' '0' '0' '0' '13' '0' '15' '0' '3' '0.5'\n",
      " '1' '60.756112']\n",
      "['Puffed Wheat' 'Q' 'C' '50' '2' '0' '0' '1' '10' '0' '50' '0' '3' '0.5'\n",
      " '1' '63.005645']\n",
      "['Quaker Oat Squares' 'Q' 'C' '100' '4' '1' '135' '2' '14' '6' '110' '25'\n",
      " '3' '1' '0.5' '49.511874']\n",
      "['Quaker Oatmeal' 'Q' 'H' '100' '5' '2' '0' '2.7' '-1' '-1' '110' '0' '1'\n",
      " '1' '0.67' '50.828392']\n"
     ]
    }
   ],
   "source": [
    "# SUUUUUUUCRE\n",
    "\n",
    "sucre = data[0,:] == 'sugars'\n",
    "    \n",
    "few_sugar = list(filter(lambda x: True if int(x[sucre]) < 10 else False, data[quaker_oats]))\n",
    "\n",
    "for i in few_sugar:\n",
    "    \n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NdsY64gl_0dw"
   },
   "source": [
    "#### Part 3\n",
    "\n",
    "Sur le m√™me dataset, √† l'aide d'une boucle `for`:\n",
    "\n",
    "\n",
    "*   Cr√©er un dictionnaire avec la moyenne de sucre par Fabricant de c√©rales\n",
    "*   Trouver la marque de c√©reales qui contient le plus de sucre par portion\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EjbTgYNbCaIj",
    "outputId": "5ab1f623-f8df-45f2-e82b-7031da73f3f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Almond Delight' 'R' 'C' '110' '2' '2' '200' '1' '14' '8' '-1' '25' '3'\n",
      "  '1' '0.75' '34.384843']\n",
      " ['Bran Chex' 'R' 'C' '90' '2' '1' '200' '4' '15' '6' '125' '25' '1' '1'\n",
      "  '0.67' '49.120253']\n",
      " ['Corn Chex' 'R' 'C' '110' '2' '0' '280' '0' '22' '3' '25' '25' '1' '1'\n",
      "  '1' '41.445019']\n",
      " ['Double Chex' 'R' 'C' '100' '2' '0' '190' '1' '18' '5' '80' '25' '3'\n",
      "  '1' '0.75' '44.330856']\n",
      " ['Muesli Raisins; Dates; & Almonds' 'R' 'C' '150' '4' '3' '95' '3' '16'\n",
      "  '11' '170' '25' '3' '1' '1' '37.136863']\n",
      " ['Muesli Raisins; Peaches; & Pecans' 'R' 'C' '150' '4' '3' '150' '3'\n",
      "  '16' '11' '170' '25' '3' '1' '1' '34.139765']\n",
      " ['Rice Chex' 'R' 'C' '110' '1' '0' '240' '0' '23' '2' '30' '25' '1' '1'\n",
      "  '1.13' '41.998933']\n",
      " ['Wheat Chex' 'R' 'C' '100' '3' '1' '230' '3' '17' '3' '115' '25' '1'\n",
      "  '1' '0.67' '49.787445']]\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "dist_man = np.unique(flatten_list(data[1:,manufacturer])) # on s√©lectionne une liste distincte des fabricants\n",
    "\n",
    "man_avg = dict.fromkeys(dist_man)\n",
    "\n",
    "for i in dist_man:\n",
    "    \n",
    "    man_avg[i] = 1\n",
    "    \n",
    "test = data[flatten_list(data[:,manufacturer] == i)][:,:]\n",
    "    \n",
    "\n",
    "print(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w1wjqra3_yZo",
    "outputId": "ea12a660-30c6-45d0-e066-c7f4848eb455"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Ralston Purina': 6.125, 'Nabisco': 1.8333333333333333, 'General Mills': 7.954545454545454, 'Kelloggs': 7.565217391304348, 'American Home Food Products': 3.0, 'Post': 8.777777777777779, 'Quaker Oats': 5.25}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHsrGdy-5AZK"
   },
   "source": [
    "## EX03\n",
    "### Dataframes\n",
    "\n",
    "Refaire l'exo du TD 4 (fichier des series) en utilisant la librairie NumPy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jIatRWhm5Xzo"
   },
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "94-12vgPXG10"
   },
   "source": [
    "## Github 101\n",
    "\n",
    "```shell\n",
    "# Repartir sur une nouvelle branche\n",
    "git checkout main\n",
    "# Mettre √† jour notre branche locale\n",
    "git pull origin main\n",
    "# Recr√©er une branche pour le TD 05\n",
    "git checkout -b branch_name_td05\n",
    "# Dupliquer le fichier du td et rajouter en suffix son nom\n",
    "cd 'chemin/m1-miage/'\n",
    "cp 'src/to_do/td_05/td_05.ipynb' 'src/done/td_05/td_05_nom_prenom.ipynb'\n",
    "# Faire le travail necessaire sur votre fichier et commit et push\n",
    "git add 'src/done/td_05/td_05_nom_prenom.ipynb'\n",
    "git commit -m 'message descriptif du travail fait'\n",
    "git push --set-upstream origin nom_branche\n",
    "# Faire une pull request\n",
    "```\n",
    "\n",
    "[>>>PULL REQUEST<<<](https://github.com/lapointe05/m1-miage/pulls)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "597Hu82gSNCZ",
    "k2lJWv62kQZA",
    "94-12vgPXG10"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
