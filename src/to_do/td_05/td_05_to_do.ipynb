{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "597Hu82gSNCZ"
      },
      "source": [
        "<img src=\"https://upload.wikimedia.org/wikipedia/fr/0/0b/Polytech_Lyon_logo.png\" alt=\"drawing\" height=\"200\"/>\n",
        "\n",
        "# Traitement de données & Programmation en Python\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EA-XK862WiMt"
      },
      "source": [
        "# TD 05\n",
        "\n",
        "Traitement de données\n",
        "\n",
        "![Good luck!](https://media.tenor.com/YoFWnXe4V3kAAAAd/may-the-odds-be-ever-in-your-favor-may-the-odds-hunger-games.gif)\n",
        " \n",
        "Elements à consulter:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Doc                                   |             Link\n",
        "--------------------------------------|------------------------------------\n",
        "Github Helper      | [>link<](#scrollTo=Github_101)\n",
        "Python en 30 jours | [>link<](https://moncoachdata.com/courses/apprendre-python-en-30-jours/)\n",
        "Get started with pandas | [>link<](https://colab.research.google.com/notebooks/snippets/pandas.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1g4EgT41MqJ"
      },
      "source": [
        "## Intro\n",
        "\n",
        "Le premier bloc devrait toujours contenir les installs/imports dont on aura besoin pour le reste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjiE8c51VoT0",
        "outputId": "2f4856bd-590b-4197-a9f0-e3a020d191eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python is awesome 👍\n"
          ]
        }
      ],
      "source": [
        "# Installs\n",
        "print(\"Python is awesome 👍\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhT-50uwz72F"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "import os\n",
        "import json\n",
        "from random import randint\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsB9ADLg8i8P",
        "outputId": "162e09d0-12e6-4112-972d-91cea118215a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2, 3)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = np.array([[1, 2, 3], [4, 5, 6]], np.int32)\n",
        "\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKow2OZ5LCXr"
      },
      "source": [
        "### Correction TD precedent\n",
        "\n",
        "Partir d'un fichier csv et alimenter la binge watch list :\n",
        "  > serie, plateform, nb_episodes, année_sortie, note\n",
        "\n",
        "Puis print :\n",
        "\n",
        "\t Nom de la série : Brooklyn Nine-Nine\n",
        "\t Année de sortie : 2013\n",
        "\n",
        "\t Nom de la série : The office\n",
        "\t Année de sortie : 2005\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "9lcQQFp3w0HE"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './netflix_titles.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [50], line 38\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[39m# file_path = \"./tv_shows_pipe.csv\"\u001b[39;00m\n\u001b[0;32m     37\u001b[0m limit \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m binge_watch_list \u001b[39m=\u001b[39m csv_to_dict(file_path,separator\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m|\u001b[39;49m\u001b[39m\"\u001b[39;49m,row_limit\u001b[39m=\u001b[39;49mlimit)\n\u001b[0;32m     39\u001b[0m \u001b[39m# for serie in binge_watch_list:\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[39m#   print(serie.get('title'))\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[39m# # Option 1 read some rows to see what we have\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[39m#               value_vars=['netflix','hulu','prime_video','disney+'])\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[39m# df2.drop(['value'], axis=1)\u001b[39;00m\n",
            "Cell \u001b[1;32mIn [50], line 13\u001b[0m, in \u001b[0;36mcsv_to_dict\u001b[1;34m(file_path, separator, row_limit)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcsv_to_dict\u001b[39m(file_path, separator\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m;\u001b[39m\u001b[39m\"\u001b[39m, row_limit\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m      2\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m    Transforms a csv file, into a list of dictionaries\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39m    Returns list[dict]\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(file_path, \u001b[39m\"\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m     14\u001b[0m         serie \u001b[39m=\u001b[39m {}\n\u001b[0;32m     15\u001b[0m         serie_list \u001b[39m=\u001b[39m []\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m    276\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    277\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m     )\n\u001b[1;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './netflix_titles.csv'"
          ]
        }
      ],
      "source": [
        "def csv_to_dict(file_path, separator=\";\", row_limit=None):\n",
        "    \"\"\"\n",
        "    Transforms a csv file, into a list of dictionaries\n",
        "\n",
        "    Arguments:\n",
        "        file_path  {str} : eg. /documents/file.txt\n",
        "        separator  {str} : separator of fields, default (;)\n",
        "        row_limit  {int} : limit the rows to be read, default None\n",
        "\n",
        "    Returns list[dict]\n",
        "    \"\"\"\n",
        "\n",
        "    with open(file_path, \"r\") as f:\n",
        "        serie = {}\n",
        "        serie_list = []\n",
        "        for i, row in enumerate(f, 1):\n",
        "            if i == 1:\n",
        "                header = row.lower().strip().replace(' ','_').split(separator)\n",
        "                # print(\"\\nHeaders:\", header,\"\\n\")\n",
        "            else:\n",
        "                data = row.lower().strip().split(separator)\n",
        "                # print(\"Data:\",data)\n",
        "                if len(header) == len(data):\n",
        "                    for j, element in enumerate(header):\n",
        "                        serie[header[j]] = data[j]\n",
        "            # print(serie)\n",
        "                serie_list.append(serie)\n",
        "                serie = {}\n",
        "            if i == row_limit:\n",
        "                break\n",
        "    return serie_list\n",
        "\n",
        "\n",
        "\n",
        "file_path = \"./netflix_titles.csv\"\n",
        "# file_path = \"./tv_shows_pipe.csv\"\n",
        "limit = None\n",
        "binge_watch_list = csv_to_dict(file_path,separator=\"|\",row_limit=limit)\n",
        "# for serie in binge_watch_list:\n",
        "#   print(serie.get('title'))\n",
        "# # Option 1 read some rows to see what we have\n",
        "\n",
        "#df = pd.DataFrame(binge_watch_list)\n",
        "#df[\"title\"] = df[\"title\"].str.title()\n",
        "#df.head(5)\n",
        "\n",
        "#column_name = 'release_year'\n",
        "#filter = df['release_year'] == '2005'\n",
        "# df.query(f\"`{column_name}` == '2003'\")\n",
        "#df[filter]\n",
        "# # serie, plateform, nb_episodes, année_sortie, note\n",
        "# df2 = df.melt(id_vars=['title','year','imdb','rotten_tomatoes'],\n",
        "#               var_name='plateform', \n",
        "#               value_vars=['netflix','hulu','prime_video','disney+'])\n",
        "# df2.drop(['value'], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2lJWv62kQZA"
      },
      "source": [
        "##EX01\n",
        "### Numpy intro\n",
        "\n",
        "* Créer un vecteur depuis la liste ['Lyon', 'Paris', 'Montpellier']\n",
        "    * Assigner le résultat à la variable `villes`\n",
        "* Créer une matrice depuis la liste de listes suivante: `[['Lyon', '69'], ['Paris', '75'], ['Montpellier','34']]`\n",
        "    * Assigner le résultat à la variable `villes_departement`\n",
        "* Assigner la taille du vecteur `villes` à la variable `v_shape`\n",
        "* Assigner la taille du tableau `villes_departement` à la variable `vd_shape` \n",
        "* Afficher les résultats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "n_jOsgGllUhC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Lyon' 'Paris' 'Montpellier']\n",
            "[['Lyon' '69']\n",
            " ['Paris' '75']\n",
            " ['Montpellier' '34']]\n",
            "3\n",
            "6\n"
          ]
        }
      ],
      "source": [
        "villes = np.array(['Lyon','Paris','Montpellier'])\n",
        "villes_department = np.array([['Lyon','69'],['Paris','75'],['Montpellier','34']])\n",
        "v_shape = np.size(villes)\n",
        "vd_shape = np.size(villes_department)\n",
        "print(villes)\n",
        "print(villes_department)\n",
        "print(v_shape)\n",
        "print(vd_shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8WBS9oVKcWs"
      },
      "source": [
        "##EX02\n",
        "### Numpy intro\n",
        "\n",
        "#### Part 1\n",
        "\n",
        "* Telecharger le dataset [cereal.csv](https://www.kaggle.com/datasets/crawford/80-cereals?select=cereal.csv) avec les colonnes suivantes :\n",
        "\n",
        "```\n",
        "['name', 'mfr', 'type', 'calories', 'protein', 'fat', 'sodium', 'fiber',\n",
        "'carbo', 'sugars', 'potass', 'vitamins', 'shelf', 'weight', 'cups',\n",
        "'rating']\n",
        "```\n",
        "\n",
        "* Afficher les deux premieres colonnes et toute les lignes\n",
        "* Afficher la dernière colonne\n",
        "* Afficher les 5 premieres lignes de la 4ème colonne\n",
        "* Assigner a une variable `corn_flakes_cals` le nombre de calories par 100 gr de céréales de la marque Corn Flakes \n",
        "* Assigner le nom de la 3eme marque sur le dataset a une variabe `third_brand`\n",
        "* Mettre dans une liste toutes les marques de céreales en utilisant `.tolist()` & afficher cette dernière\n",
        "* Mettre les 10 premiere marques dans le fichiers dans une liste `first_ten_brands` & afficher cette dernière\n",
        "> bonus \n",
        "* Convertir la colonne ratings en décimale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTDsqYIgpitx",
        "outputId": "d2cd6d75-cf4d-4421-c34f-097def1f3b94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(78, 16)\n"
          ]
        }
      ],
      "source": [
        "# Code here\n",
        "file_path = \"./cereal.csv\"\n",
        "data = np.genfromtxt(file_path,delimiter=',',dtype='str',autostrip=True)\n",
        "# Deux premières colonnes et toutes les lignes\n",
        "# print(data[:,:1])\n",
        "# Dernière colonne\n",
        "# print(data[data.shape[0]-1,:])\n",
        "# 5 premières lignes de la 4è colonne\n",
        "# print(data[:4,3])\n",
        "\n",
        "corn_flakes = (data[:,0] == 'Corn Flakes')\n",
        "corn_flakes_cals = data[corn_flakes, 3]\n",
        "# print(corn_flakes_cals)\n",
        "\n",
        "third_brand = data[3,0]\n",
        "# print(third_brand)\n",
        "\n",
        "brands = data[1:,0].tolist()\n",
        "# print(brands)\n",
        "\n",
        "first_ten_brands = data[1:11,0]\n",
        "# print(first_ten_brands)\n",
        "\n",
        "# Bonus\n",
        "#print(data[1:,15].astype(float))\n",
        "print(data.shape) ## retourne (lignes, colonnes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BaW1VvWXCS7"
      },
      "source": [
        "#### Part 2\n",
        "\n",
        "* **Extraire** la premiere colonne du dataset et **la comparer** à la valeur `'Special K'` Assigner le resultat à une variable `special_k`\n",
        "* Executer la commande `print(data[special_k])`\n",
        "> bonus\n",
        "* Faire la meme chose mais le filtre verifie que le manufacturer est Quaker Oats (lire la doc du dataset 😉)\n",
        "> bonus ²\n",
        "* Print le nom des marques dont le fabricant est `Quaker Oats` et qui contiennent **moins** de `10 grammes` de sucre par portion\n",
        "* Ajouter une marque de céréales \"healthy\" avec 0 grammes de sucre au dataset "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GuXFyhUepnMc",
        "outputId": "6ec9c05d-a76c-4d8f-8c48-6285d32b6953"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['name' 'mfr' 'type' ... 'weight' 'cups' 'rating']\n",
            " ['100% Bran' 'N' 'C' ... '1' '0.33' '68.402973']\n",
            " ['100% Natural Bran' 'Q' 'C' ... '1' '1' '33.983679']\n",
            " ...\n",
            " ['Wheaties' 'G' 'C' ... '1' '1' '51.592193']\n",
            " ['Wheaties Honey Gold' 'G' 'C' ... '1' '0.75' '36.187559']\n",
            " ['healthy' '' '' ... '' '' '']]\n"
          ]
        }
      ],
      "source": [
        "special_k = (data[1:,0] == 'Special K')\n",
        "# print(special_k)\n",
        "\n",
        "# Bonus\n",
        "quaker_oaks = (data[1:,1] == 'Q')\n",
        "# print(quaker_oaks)\n",
        "\n",
        "# Bonus 2\n",
        "quaker_dix_g = (quaker_oaks & (data[1:,9].astype(int) <= 10))\n",
        "# print(quaker_dix_g)\n",
        "\n",
        "healthy = np.array(['healthy','','','','','','','','','0','','','','','',''])\n",
        "data = np.vstack([data,healthy])\n",
        "print(data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdsY64gl_0dw"
      },
      "source": [
        "#### Part 3\n",
        "\n",
        "Sur le même dataset, à l'aide d'une boucle `for`:\n",
        "\n",
        "\n",
        "*   Créer un dictionnaire avec la moyenne de sucre par Fabricant de cérales\n",
        "*   Trouver la marque de céreales qui contient le plus de sucre par portion\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjbTgYNbCaIj",
        "outputId": "5ab1f623-f8df-45f2-e82b-7031da73f3f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'Ralston Purina': 6.125, 'Nabisco': 1.8333333333333333, 'General Mills': 7.954545454545454, 'Kelloggs': 7.565217391304348, 'American Home Food Products': 3.0, 'Post': 8.777777777777779, 'Quaker Oats': 5.25}\n",
            "Post\n"
          ]
        }
      ],
      "source": [
        "brand_dic = {\"Ralston Purina\": \"R\", \"Nabisco\": \"N\", \"General Mills\": \"G\", \"Kelloggs\": \"K\", \"American Home Food Products\": \"A\", \"Post\": \"P\", \"Quaker Oats\": \"Q\"}\n",
        "brand_mean_sugar_dic = {}\n",
        "for brand in brand_dic:\n",
        "    brand_mean_sugar_dic[brand] = np.mean((data[data[:,1] == brand_dic[brand]])[:,9].astype(float))\n",
        "print(brand_mean_sugar_dic)\n",
        "print(f\"{max(brand_mean_sugar_dic, key=brand_mean_sugar_dic.get)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1wjqra3_yZo",
        "outputId": "ea12a660-30c6-45d0-e066-c7f4848eb455"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'Ralston Purina': 6.125, 'Nabisco': 1.8333333333333333, 'General Mills': 7.954545454545454, 'Kelloggs': 7.565217391304348, 'American Home Food Products': 3.0, 'Post': 8.777777777777779, 'Quaker Oats': 5.25}\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHsrGdy-5AZK"
      },
      "source": [
        "## EX03\n",
        "### Dataframes\n",
        "\n",
        "Refaire l'exo du TD 4 (fichier des series) en utilisant la librairie NumPy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "jIatRWhm5Xzo"
      },
      "outputs": [
        {
          "ename": "UnicodeDecodeError",
          "evalue": "'charmap' codec can't decode byte 0x81 in position 135: character maps to <undefined>",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [9], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m dict_series \u001b[39m=\u001b[39m {}\n\u001b[0;32m      3\u001b[0m file_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m./tv_shows.csv\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> 4\u001b[0m data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mgenfromtxt(file_path,delimiter\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m|\u001b[39;49m\u001b[39m'\u001b[39;49m,dtype\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mstr\u001b[39;49m\u001b[39m'\u001b[39;49m,autostrip\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m      5\u001b[0m \u001b[39m# lines = data.readlines()\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39m# for line in lines:\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39m#     element = line.split(\"|\")\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39m#     dict_series[liste_series[i][0]]={\"Titre\" : liste_series[i][1], \"Plateforme\" : plat, \"Année de sortie\" : liste_series[i][2], \"Note\" : liste_series[i][4]}        #nombre d'épisodes pas dans le dictionnaire pcq info pas présente dans le fichier csv\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39m# print(dict_series)\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\nicol\\anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py:2059\u001b[0m, in \u001b[0;36mgenfromtxt\u001b[1;34m(fname, dtype, comments, delimiter, skip_header, skip_footer, converters, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise, max_rows, encoding, like)\u001b[0m\n\u001b[0;32m   2056\u001b[0m append_to_invalid \u001b[39m=\u001b[39m invalid\u001b[39m.\u001b[39mappend\n\u001b[0;32m   2058\u001b[0m \u001b[39m# Parse each line\u001b[39;00m\n\u001b[1;32m-> 2059\u001b[0m \u001b[39mfor\u001b[39;00m (i, line) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(itertools\u001b[39m.\u001b[39mchain([first_line, ], fhd)):\n\u001b[0;32m   2060\u001b[0m     values \u001b[39m=\u001b[39m split_line(line)\n\u001b[0;32m   2061\u001b[0m     nbvalues \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(values)\n",
            "File \u001b[1;32mc:\\Users\\nicol\\anaconda3\\lib\\encodings\\cp1252.py:23\u001b[0m, in \u001b[0;36mIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, final\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m---> 23\u001b[0m     \u001b[39mreturn\u001b[39;00m codecs\u001b[39m.\u001b[39;49mcharmap_decode(\u001b[39minput\u001b[39;49m,\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merrors,decoding_table)[\u001b[39m0\u001b[39m]\n",
            "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x81 in position 135: character maps to <undefined>"
          ]
        }
      ],
      "source": [
        "liste_series = []\n",
        "dict_series = {}\n",
        "file_path = \"./tv_shows.csv\"\n",
        "data = np.genfromtxt(file_path,delimiter='|',dtype='str',autostrip=True)\n",
        "# for i in range(1,data.shape[0]):\n",
        "#     plat = \"\"\n",
        "#     if data[i,6] == \"1\":\n",
        "#         plat += \"Netflix\"\n",
        "#     if data[i,7] == \"1\":\n",
        "#         plat += \" Hulu\"\n",
        "#     if data[i,8] == \"1\":\n",
        "#         plat += \" Prime Video\"\n",
        "#     if data[i,9] == \"1\":\n",
        "#         plat += \" Dinsney +\"\n",
        "#     dict_series[data[i,0]={\"Titre\" : data[i,1], \"Plateforme\" : plat, \"Année de sortie\" : data[i,2], \"Note\" : data[i,4]}        #nombre d'épisodes pas dans le dictionnaire pcq info pas présente dans le fichier csv\n",
        "# print(dict_series)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94-12vgPXG10"
      },
      "source": [
        "## Github 101\n",
        "\n",
        "```shell\n",
        "# Repartir sur une nouvelle branche\n",
        "git checkout main\n",
        "# Mettre à jour notre branche locale\n",
        "git pull origin main\n",
        "# Recréer une branche pour le TD 05\n",
        "git checkout -b branch_name_td05\n",
        "# Dupliquer le fichier du td et rajouter en suffix son nom\n",
        "cd 'chemin/m1-miage/'\n",
        "cp 'src/to_do/td_05/td_05.ipynb' 'src/done/td_05/td_05_nom_prenom.ipynb'\n",
        "# Faire le travail necessaire sur votre fichier et commit et push\n",
        "git add 'src/done/td_05/td_05_nom_prenom.ipynb'\n",
        "git commit -m 'message descriptif du travail fait'\n",
        "git push --set-upstream origin nom_branche\n",
        "# Faire une pull request\n",
        "```\n",
        "\n",
        "[>>>PULL REQUEST<<<](https://github.com/lapointe05/m1-miage/pulls)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "597Hu82gSNCZ",
        "k2lJWv62kQZA",
        "94-12vgPXG10"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "07878c45a9b18549480ac2f9ecb8f16bb7162456cf654f6757bcff1d6396abe7"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
