{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "597Hu82gSNCZ",
        "k2lJWv62kQZA",
        "94-12vgPXG10"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lapointe05/m1-miage/blob/IssamBejja/td_05_to_do.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "597Hu82gSNCZ"
      },
      "source": [
        "<img src=\"https://upload.wikimedia.org/wikipedia/fr/0/0b/Polytech_Lyon_logo.png\" alt=\"drawing\" height=\"200\"/>\n",
        "\n",
        "# Traitement de donn√©es & Programmation en Python\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TD 05\n",
        "\n",
        "Traitement de donn√©es\n",
        "\n",
        "![Good luck!](https://media.tenor.com/YoFWnXe4V3kAAAAd/may-the-odds-be-ever-in-your-favor-may-the-odds-hunger-games.gif)\n",
        " \n",
        "Elements √† consulter:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Doc                                   |             Link\n",
        "--------------------------------------|------------------------------------\n",
        "Github Helper      | [>link<](#scrollTo=Github_101)\n",
        "Python en 30 jours | [>link<](https://moncoachdata.com/courses/apprendre-python-en-30-jours/)\n",
        "Get started with pandas | [>link<](https://colab.research.google.com/notebooks/snippets/pandas.ipynb)"
      ],
      "metadata": {
        "id": "EA-XK862WiMt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Intro\n",
        "\n",
        "Le premier bloc devrait toujours contenir les installs/imports dont on aura besoin pour le reste"
      ],
      "metadata": {
        "id": "Y1g4EgT41MqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installs\n",
        "print(\"Python is awesome üëç\")"
      ],
      "metadata": {
        "id": "yjiE8c51VoT0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f4856bd-590b-4197-a9f0-e3a020d191eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python is awesome üëç\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import os\n",
        "import json\n",
        "from random import randint\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "JhT-50uwz72F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array([[1, 2, 3], [4, 5, 6]], np.int32)\n",
        "\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsB9ADLg8i8P",
        "outputId": "162e09d0-12e6-4112-972d-91cea118215a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Correction TD precedent\n",
        "\n",
        "Partir d'un fichier csv et alimenter la binge watch list :\n",
        "  > serie, plateform, nb_episodes, ann√©e_sortie, note\n",
        "\n",
        "Puis print :\n",
        "\n",
        "\t Nom de la s√©rie : Brooklyn Nine-Nine\n",
        "\t Ann√©e de sortie : 2013\n",
        "\n",
        "\t Nom de la s√©rie : The office\n",
        "\t Ann√©e de sortie : 2005\n"
      ],
      "metadata": {
        "id": "iKow2OZ5LCXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def csv_to_dict(file_path, separator=\";\", row_limit=None):\n",
        "    \"\"\"\n",
        "    Transforms a csv file, into a list of dictionaries\n",
        "\n",
        "    Arguments:\n",
        "        file_path  {str} : eg. /documents/file.txt\n",
        "        separator  {str} : separator of fields, default (;)\n",
        "        row_limit  {int} : limit the rows to be read, default None\n",
        "\n",
        "    Returns list[dict]\n",
        "    \"\"\"\n",
        "\n",
        "    with open(file_path, \"r\") as f:\n",
        "        serie = {}\n",
        "        serie_list = []\n",
        "        for i, row in enumerate(f, 1):\n",
        "            if i == 1:\n",
        "                header = row.lower().strip().replace(' ','_').split(separator)\n",
        "                # print(\"\\nHeaders:\", header,\"\\n\")\n",
        "            else:\n",
        "                data = row.lower().strip().split(separator)\n",
        "                # print(\"Data:\",data)\n",
        "                if len(header) == len(data):\n",
        "                    for j, element in enumerate(header):\n",
        "                        serie[header[j]] = data[j]\n",
        "            # print(serie)\n",
        "                serie_list.append(serie)\n",
        "                serie = {}\n",
        "            if i == row_limit:\n",
        "                break\n",
        "    return serie_list\n",
        "\n",
        "\n",
        "\n",
        "file_path = \"./netflix_titles.csv\"\n",
        "# file_path = \"./tv_shows_pipe.csv\"\n",
        "limit = None\n",
        "binge_watch_list = csv_to_dict(file_path,separator=\"|\",row_limit=limit)\n",
        "# for serie in binge_watch_list:\n",
        "#   print(serie.get('title'))\n",
        "# # Option 1 read some rows to see what we have\n",
        "\n",
        "df = pd.DataFrame(binge_watch_list)\n",
        "df[\"title\"] = df[\"title\"].str.title()\n",
        "df.head(5)\n",
        "\n",
        "column_name = 'release_year'\n",
        "filter = df['release_year'] == '2005'\n",
        "# df.query(f\"`{column_name}` == '2003'\")\n",
        "df[filter]\n",
        "# # serie, plateform, nb_episodes, ann√©e_sortie, note\n",
        "# df2 = df.melt(id_vars=['title','year','imdb','rotten_tomatoes'],\n",
        "#               var_name='plateform', \n",
        "#               value_vars=['netflix','hulu','prime_video','disney+'])\n",
        "# df2.drop(['value'], axis=1)"
      ],
      "metadata": {
        "id": "9lcQQFp3w0HE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##EX01\n",
        "### Numpy intro\n",
        "\n",
        "* Cr√©er un vecteur depuis la liste ['Lyon', 'Paris', 'Montpellier']\n",
        "    * Assigner le r√©sultat √† la variable `villes`\n",
        "* Cr√©er une matrice depuis la liste de listes suivante: `[['Lyon', '69'], ['Paris', '75'], ['Montpellier','34']]`\n",
        "    * Assigner le r√©sultat √† la variable `villes_departement`\n",
        "* Assigner la taille du vecteur `villes` √† la variable `v_shape`\n",
        "* Assigner la taille du tableau `villes_departement` √† la variable `vd_shape` \n",
        "* Afficher les r√©sultats"
      ],
      "metadata": {
        "id": "k2lJWv62kQZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code here\n",
        "import numpy as np\n",
        "# Liste\n",
        "liste=['Lyon', 'Paris', 'Montpellier']\n",
        "#print(type(liste))\n",
        "\n",
        "# List to vecteur\n",
        "villes = np.array(liste)\n",
        "print(villes)\n",
        "\n",
        "# Matrice\n",
        "villes_departement = np.array([['Lyon', '69'], ['Paris', '75'], ['Montpellier','34']])\n",
        "#print(type(villes_departement))\n",
        "print(villes_departement)\n",
        "\n",
        "#taille vecteur villes\n",
        "v_shape = villes.shape\n",
        "print(v_shape)\n",
        "\n",
        "#taille matrice villes_departement\n",
        "vd_shape = villes_departement.shape\n",
        "print(vd_shape)"
      ],
      "metadata": {
        "id": "n_jOsgGllUhC",
        "outputId": "7e66be88-7c81-4b4b-e4a9-a06db9b4e0d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Lyon' 'Paris' 'Montpellier']\n",
            "[['Lyon' '69']\n",
            " ['Paris' '75']\n",
            " ['Montpellier' '34']]\n",
            "(3,)\n",
            "(3, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##EX02\n",
        "### Numpy intro\n",
        "\n",
        "#### Part 1\n",
        "\n",
        "* Telecharger le dataset [cereal.csv](https://www.kaggle.com/datasets/crawford/80-cereals?select=cereal.csv) avec les colonnes suivantes :\n",
        "\n",
        "```\n",
        "['name', 'mfr', 'type', 'calories', 'protein', 'fat', 'sodium', 'fiber',\n",
        "'carbo', 'sugars', 'potass', 'vitamins', 'shelf', 'weight', 'cups',\n",
        "'rating']\n",
        "```\n",
        "\n",
        "* Afficher les deux premieres colonnes et toute les lignes\n",
        "* Afficher la derni√®re colonne\n",
        "* Afficher les 5 premieres lignes de la 4√®me colonne\n",
        "* Assigner a une variable `corn_flakes_cals` le nombre de calories par 100 gr de c√©r√©ales de la marque Corn Flakes \n",
        "* Assigner le nom de la 3eme marque sur le dataset a une variabe `third_brand`\n",
        "* Mettre dans une liste toutes les marques de c√©reales en utilisant `.tolist()` & afficher cette derni√®re\n",
        "* Mettre les 10 premiere marques dans le fichiers dans une liste `first_ten_brands` & afficher cette derni√®re\n",
        "> bonus \n",
        "* Convertir la colonne ratings en d√©cimale"
      ],
      "metadata": {
        "id": "Q8WBS9oVKcWs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code here\n",
        "\n",
        "import numpy as np\n",
        "#\n",
        "file_path = \"./cereal.csv\"\n",
        "data = np.genfromtxt(file_path,dtype='str', delimiter=',', autostrip=True, skip_header=1, max_rows=50)\n",
        "# print(data)\n",
        "\n",
        "#selectionne les 2 premieres colonnes (ctrl / pour comm, ctrl Entree pour run)\n",
        "col2e = data[:,:2]\n",
        "print(\"2 premieres colonnes\",col2e)\n",
        "# print(data.shape) ## retourne (lignes, colonnes)\n",
        "\n",
        "#Affiche 5e lignes et 4e colonne\n",
        "l5e_col4 = data[:5,4]\n",
        "print(\"5 premi√®res lignes et 4e colonne: \",l5e_col4)\n",
        "print(data.shape) ## retourne (lignes, colonnes)\n",
        "\n",
        "#Assigner a une variable corn_flakes_cals le nombre de calories par 100 gr de c√©r√©ales de la marque Corn Flakes\n",
        "\n",
        "corn_flakes = (data[:,0] == 'Corn Flakes')\n",
        "# print(corn_flakes)\n",
        "\n",
        "corn_flakes_cals = data[corn_flakes,3]\n",
        "print(corn_flakes_cals)\n",
        "\n",
        "# Assigner le nom de la 3eme marque sur le dataset a une variabe third_brand\n",
        "\n",
        "third_brand = data[2,0]\n",
        "print(third_brand)\n",
        "\n",
        "#Mettre dans une liste toutes les marques de c√©reales en utilisant .tolist() & afficher cette derni√®re\n",
        "\n",
        "brand_list = data[:,0].tolist()\n",
        "print(\"Liste des marques: \",brand_list)\n",
        "\n",
        "#Mettre les 10 premiere marques dans le fichiers dans une liste first_ten_brands & afficher cette derni√®re\n",
        "\n",
        "first_ten_brands = data[:10,0].tolist()\n",
        "print(\"Liste des 10 premi√®res marques: \",first_ten_brands)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTDsqYIgpitx",
        "outputId": "d0eef9a7-6bba-47be-8a77-9b8aa5deea95"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2 premieres colonnes [['100% Bran' 'N']\n",
            " ['100% Natural Bran' 'Q']\n",
            " ['All-Bran' 'K']\n",
            " ['All-Bran with Extra Fiber' 'K']\n",
            " ['Almond Delight' 'R']\n",
            " ['Apple Cinnamon Cheerios' 'G']\n",
            " ['Apple Jacks' 'K']\n",
            " ['Basic 4' 'G']\n",
            " ['Bran Chex' 'R']\n",
            " ['Bran Flakes' 'P']\n",
            " [\"Cap'n'Crunch\" 'Q']\n",
            " ['Cheerios' 'G']\n",
            " ['Cinnamon Toast Crunch' 'G']\n",
            " ['Clusters' 'G']\n",
            " ['Cocoa Puffs' 'G']\n",
            " ['Corn Chex' 'R']\n",
            " ['Corn Flakes' 'K']\n",
            " ['Corn Pops' 'K']\n",
            " ['Count Chocula' 'G']\n",
            " [\"Cracklin' Oat Bran\" 'K']\n",
            " ['Cream of Wheat (Quick)' 'N']\n",
            " ['Crispix' 'K']\n",
            " ['Crispy Wheat & Raisins' 'G']\n",
            " ['Double Chex' 'R']\n",
            " ['Froot Loops' 'K']\n",
            " ['Frosted Flakes' 'K']\n",
            " ['Frosted Mini-Wheats' 'K']\n",
            " ['Fruit & Fibre Dates; Walnuts; and Oats' 'P']\n",
            " ['Fruitful Bran' 'K']\n",
            " ['Fruity Pebbles' 'P']\n",
            " ['Golden Crisp' 'P']\n",
            " ['Golden Grahams' 'G']\n",
            " ['Grape Nuts Flakes' 'P']\n",
            " ['Grape-Nuts' 'P']\n",
            " ['Great Grains Pecan' 'P']\n",
            " ['Honey Graham Ohs' 'Q']\n",
            " ['Honey Nut Cheerios' 'G']\n",
            " ['Honey-comb' 'P']\n",
            " ['Just Right Crunchy  Nuggets' 'K']\n",
            " ['Just Right Fruit & Nut' 'K']\n",
            " ['Kix' 'G']\n",
            " ['Life' 'Q']\n",
            " ['Lucky Charms' 'G']\n",
            " ['Maypo' 'A']\n",
            " ['Muesli Raisins; Dates; & Almonds' 'R']\n",
            " ['Muesli Raisins; Peaches; & Pecans' 'R']\n",
            " ['Mueslix Crispy Blend' 'K']\n",
            " ['Multi-Grain Cheerios' 'G']\n",
            " ['Nut&Honey Crunch' 'K']\n",
            " ['Nutri-Grain Almond-Raisin' 'K']]\n",
            "5 premi√®res lignes et 4e colonne:  ['4' '3' '4' '4' '2']\n",
            "(50, 16)\n",
            "['100']\n",
            "All-Bran\n",
            "Liste des marques:  ['100% Bran', '100% Natural Bran', 'All-Bran', 'All-Bran with Extra Fiber', 'Almond Delight', 'Apple Cinnamon Cheerios', 'Apple Jacks', 'Basic 4', 'Bran Chex', 'Bran Flakes', \"Cap'n'Crunch\", 'Cheerios', 'Cinnamon Toast Crunch', 'Clusters', 'Cocoa Puffs', 'Corn Chex', 'Corn Flakes', 'Corn Pops', 'Count Chocula', \"Cracklin' Oat Bran\", 'Cream of Wheat (Quick)', 'Crispix', 'Crispy Wheat & Raisins', 'Double Chex', 'Froot Loops', 'Frosted Flakes', 'Frosted Mini-Wheats', 'Fruit & Fibre Dates; Walnuts; and Oats', 'Fruitful Bran', 'Fruity Pebbles', 'Golden Crisp', 'Golden Grahams', 'Grape Nuts Flakes', 'Grape-Nuts', 'Great Grains Pecan', 'Honey Graham Ohs', 'Honey Nut Cheerios', 'Honey-comb', 'Just Right Crunchy  Nuggets', 'Just Right Fruit & Nut', 'Kix', 'Life', 'Lucky Charms', 'Maypo', 'Muesli Raisins; Dates; & Almonds', 'Muesli Raisins; Peaches; & Pecans', 'Mueslix Crispy Blend', 'Multi-Grain Cheerios', 'Nut&Honey Crunch', 'Nutri-Grain Almond-Raisin']\n",
            "Liste des 10 premi√®res marques:  ['100% Bran', '100% Natural Bran', 'All-Bran', 'All-Bran with Extra Fiber', 'Almond Delight', 'Apple Cinnamon Cheerios', 'Apple Jacks', 'Basic 4', 'Bran Chex', 'Bran Flakes']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Part 2\n",
        "\n",
        "* **Extraire** la premiere colonne du dataset et **la comparer** √† la valeur `'Special K'` Assigner le resultat √† une variable `special_k`\n",
        "* Executer la commande `print(data[special_k])`\n",
        "> bonus\n",
        "* Faire la meme chose mais le filtre verifie que le manufacturer est Quaker Oats (lire la doc du dataset üòâ)\n",
        "> bonus ¬≤\n",
        "* Print le nom des marques dont le fabricant est `Quaker Oats` et qui contiennent **moins** de `10 grammes` de sucre par portion\n",
        "* Ajouter une marque de c√©r√©ales \"healthy\" avec 0 grammes de sucre au dataset "
      ],
      "metadata": {
        "id": "8BaW1VvWXCS7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code here\n",
        "import numpy as np\n",
        "\n",
        "# data access\n",
        "file_path = \"./cereal.csv\"\n",
        "data = np.genfromtxt(file_path,dtype='str', delimiter=',', autostrip=True, skip_header=1)\n",
        "\n",
        "# Extraire la premiere colonne du dataset et la comparer √† la valeur 'Special K' Assigner le resultat √† une variable special_k\n",
        "special_k = (data[:,0] == 'Special K')\n",
        "# print(special_k)\n",
        "print(\"Special K cereal\",data[special_k])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GuXFyhUepnMc",
        "outputId": "ee06014e-da77-449d-aaf8-2670711ea96d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Special K cereal [['Special K' 'K' 'C' '110' '6' '0' '230' '1' '16' '3' '55' '25' '1' '1'\n",
            "  '1' '53.131324']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Part 3\n",
        "\n",
        "Sur le m√™me dataset, √† l'aide d'une boucle `for`:\n",
        "\n",
        "\n",
        "*   Cr√©er un dictionnaire avec la moyenne de sucre par Fabricant de c√©rales\n",
        "*   Trouver la marque de c√©reales qui contient le plus de sucre par portion\n",
        "\n"
      ],
      "metadata": {
        "id": "NdsY64gl_0dw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "file_path = \"./cereal.csv\"\n",
        "data=np.genfromtxt(file_path, dtype='str', delimiter=',', autostrip=True, skip_header=1,max_rows=None)\n",
        "\n",
        "sugar_dict = {}\n",
        "for i in np.unique(data[:,1]):\n",
        "    sugar_dict[i] = np.mean(data[data[:,1] == i,9].astype(float))\n",
        "print(sugar_dict)\n",
        "\n",
        "print(data[data[:,9].astype(float).argmax(),0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjbTgYNbCaIj",
        "outputId": "3e403b17-3bee-49cd-8161-782ef2e5a220"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'A': 3.0, 'G': 7.954545454545454, 'K': 7.565217391304348, 'N': 1.8333333333333333, 'P': 8.777777777777779, 'Q': 5.25, 'R': 6.125}\n",
            "Golden Crisp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1wjqra3_yZo",
        "outputId": "ea12a660-30c6-45d0-e066-c7f4848eb455"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Ralston Purina': 6.125, 'Nabisco': 1.8333333333333333, 'General Mills': 7.954545454545454, 'Kelloggs': 7.565217391304348, 'American Home Food Products': 3.0, 'Post': 8.777777777777779, 'Quaker Oats': 5.25}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EX03\n",
        "### Dataframes\n",
        "\n",
        "Refaire l'exo du TD 4 (fichier des series) en utilisant la librairie NumPy.\n"
      ],
      "metadata": {
        "id": "SHsrGdy-5AZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "file_path = \"./netflix_titles.csv\"\n",
        "data=np.genfromtxt(file_path, dtype='str', delimiter=',', autostrip=True, skip_header=1,max_rows=None)\n",
        "\n",
        "series = {}\n",
        "for serie in data:\n",
        "    if serie[1]==\"TV Show\":\n",
        "        series[serie[0]]= {'Titre de la serie' : serie[2], 'Plateforme de streaming' : serie[8], \"Nombre d √©pisodes\" : serie[9], 'Ann√©e de sortie': serie[7], 'note /10 \\n' : serie[5]}\n",
        "\n",
        "print(series)"
      ],
      "metadata": {
        "id": "jIatRWhm5Xzo",
        "outputId": "340282ee-fd76-4d45-85e1-eeccd09e8d0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-c4f55f2fd286>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./netflix_titles.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'str'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mautostrip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_header\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_rows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mseries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mgenfromtxt\u001b[0;34m(fname, dtype, comments, delimiter, skip_header, skip_footer, converters, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise, max_rows, encoding, like)\u001b[0m\n\u001b[1;32m   1791\u001b[0m             \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1792\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1793\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1794\u001b[0m             \u001b[0mfid_ctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontextlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1795\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[1;32m    531\u001b[0m                                       encoding=encoding, newline=newline)\n\u001b[1;32m    532\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s not found.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: ./netflix_titles.csv not found."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Github 101\n",
        "\n",
        "```shell\n",
        "# Repartir sur une nouvelle branche\n",
        "git checkout main\n",
        "# Mettre √† jour notre branche locale\n",
        "git pull origin main\n",
        "# Recr√©er une branche pour le TD 05\n",
        "git checkout -b branch_name_td05\n",
        "# Dupliquer le fichier du td et rajouter en suffix son nom\n",
        "cd 'chemin/m1-miage/'\n",
        "cp 'src/to_do/td_05/td_05.ipynb' 'src/done/td_05/td_05_nom_prenom.ipynb'\n",
        "# Faire le travail necessaire sur votre fichier et commit et push\n",
        "git add 'src/done/td_05/td_05_nom_prenom.ipynb'\n",
        "git commit -m 'message descriptif du travail fait'\n",
        "git push --set-upstream origin nom_branche\n",
        "# Faire une pull request\n",
        "```\n",
        "\n",
        "[>>>PULL REQUEST<<<](https://github.com/lapointe05/m1-miage/pulls)\n"
      ],
      "metadata": {
        "id": "94-12vgPXG10"
      }
    }
  ]
}